<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Cherry picking with Reinforcement learning">
  <meta name="keywords" content="Dyanmics Fine Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Cherrybot</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-4Y34PZ3XBE"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-4Y34PZ3XBE');
  </script>

  <script>
    function updateSingleVideo() {
      var demo = document.getElementById("single-menu-demos").value;
      var task = document.getElementById("single-menu-tasks").value;
      var inst = document.getElementById("single-menu-instances").value;

      console.log("single", demo, task, inst)

      var video = document.getElementById("single-task-result-video");
      video.src = "https://homes.cs.washington.edu/~mshr/cliport/results_web/" + 
                  task + 
                  "-two_stream_full_clip_lingunet_lat_transporter-n" + 
                  demo + 
                  "-train/videos/" + 
                  task +
                  "-0000" + 
                  inst + 
                  ".mp4";
      video.playbackRate = 2.0;
      video.play();
    }

    function updateMultiVideo() {
      var demo = document.getElementById("multi-menu-demos").value;
      var task = document.getElementById("multi-menu-tasks").value;
      var inst = document.getElementById("multi-menu-instances").value;

      console.log("multi", demo, task, inst)

      var video = document.getElementById("multi-task-result-video");
      video.src = "https://homes.cs.washington.edu/~mshr/cliport/results_web/" + 
                  task + 
                  "-two_stream_full_clip_lingunet_lat_transporter-n" + 
                  demo + 
                  "-train/videos/multi-language-conditioned-" + 
                  task +
                  "-0000" + 
                  inst + 
                  ".mp4";
      video.playbackRate = 2.0;
      video.play();
    }

  </script>



  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          Previous Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" target="_blank" href="https://personalrobotics.cs.washington.edu/publications/ke2021grasping.pdf">
            Grasping with Chopsticks 
          </a>
          <a class="navbar-item" target="_blank" href="https://personalrobotics.cs.washington.edu/publications/ke2020teleop.pdf">
            Telemanipulation with Chopsticks
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Cherry picking with Reinforcement learning</h1>
          <!--
          <h3 class="title is-4 conference-authors"><a target="_blank" href="https://www.robot-learning.org/">CoRL 2021</a></h3>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank" href="https://mohitshridhar.com/">Mohit Shridhar</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="http://lucasmanuelli.com/">Lucas Manuelli</a><sup>2</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a><sup>1, 2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Washington,</span>
            <span class="author-block"><sup>2</sup>NVIDIA</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
                <span class="link-block">
                <a target="_blank" href="https://arxiv.org/pdf/2109.12098.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
               <span class="link-block">
                <a target="_blank" href="https://drive.google.com/file/d/1xzG5e1XF958HPuD_FZTiKROd9AQyd1fS/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a target="_blank" href="https://youtu.be/UdzoagBgWTA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> 
               <span class="link-block">
                <a target="_blank" href="https://github.com/cliport/cliport"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
            -->
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container">
    <div class="wrappable_list">
      <span>
        <image src="media/gifs/pluck_from_tree.gif" alt="GIF of robot picking cherry from a tree" />
      </span>
      <span>
        <image src="media/gifs/grab_from_sand.gif" alt="GIF of robot picking cherry off of shifting sand" />
      </span>
      <span>
        <image src="media/gifs/hand_disturbance.gif" alt="GIF of robot grabbing ball that is being shaken around by a hand" />
      </span>
      <span>
        <image src="media/gifs/duck.gif" alt="GIF of robot picking up rubber duck" />
      </span>
      <span>
        <image src="media/gifs/grapes.gif" alt="Gif of robot picking up grape from a pile of other grapes" />
      </span>
    </div>
  </div>

  <!-- <div class="container is-fullhd">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop height="100%">
        <source src="https://cliport.github.io/media/videos/10sim_web_teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
      </br>
        <span class="dcliport">Cherrybot</span> is an model-free reinforcement-learning agent that could generalize to varying perception noise; objects with different shape, size, texture; dynamic disturbance; objects with varying fluid support in zero-shot, <b>without further learning</b>.
      </h2>
    </div>
  </div> -->
</section>


<!-- 
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop height="100%">
            <source src="https://cliport.github.io/media/videos/1_folding.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop height="100%">
            <source src="https://cliport.github.io/media/videos/4_chess.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop height="100%">
            <source src="https://cliport.github.io/media/videos/3_packing.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> 
-->


<!--
<h2 class="subtitle has-text-centered">
  </br>
    We learn <b>one multi-task policy</b> for 9 real-world tasks including folding cloths, sweeping beans etc. with just <b>179</b> image-action training pairs.
</h2>
-->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">The Problem</h2>
        <div class="content has-text-justified">
          <p>
            How can we automate cherry-picking üçí when the wind is blowing, the branch is shaking and the cherries are trembling? 
            This task is an example of grasping <b>without rigid-surface support</b>: the problem is inherently dynamic as 
            any contact with the object might cause it to move and further disturb its movement. The difficulty becomes more 
            prominent for fine manipulation of small objects, as the perception error and sensor noise dominate and grasps can 
            hardly be precise. Similar challenges arise in everyday interactions: to remove broken shells from flowing egg whites, 
            to retrieve earrings from the hanging rack, and for surgeons to remove clots from deformable organs. Given the ubiquitous 
            nature of these problems, developing robotic solutions to automate these has immense practical and economic value. 
          </p>
          <div>
            <p>
            Similar challenges arise in everyday interactions: to remove shells 
            from flowing egg whites,
            </p>
            <div class="horizontal_list">
              <span class="list_image">
                <img src="media/imgs/egg-shells.jpg" class="interpolation-image" alt="Egg Shell" style="max-width: 100%; max-height: 100%;" />
                <p class="image_caption">Picking up egg yolks</p>
              </span>
              <span class="list_image">
                <img src="media/imgs/noodles.jpg" class="interpolation-image" alt="noodles" />
                <p class="image_caption">Picking up noodles</p>
              </span>
              <span class="list_image">
                <img src="media/imgs/surgery.jpg" class="interpolation-image" alt="noodles" />
                <p class="image_caption">Removing clots from deformable organs</p>
              </span>
            </div>
            
          to grasp noodles from soup,
          
            and for surgeons to remove clots from deformable organs. 
            Given the ubiquitous nature of these problems, developing robotic 
            solutions to automate these has immense practical and economic value.
          </div>
        </div>
      </div>
      
    </div>
    <!--/ Abstract. -->

  </div>

    <!-- Paper video. -->
    <!--
    </br>
    </br>
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
        <h2 class="title is-3">Video</h2>
        <div class="publhttps://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.thekitchn.com%2Fnostress-solution-how-to-get-t-108285&psig=AOvVaw0y0Ig-QftM9qU6kjN_uCnS&ust=1668638482016000&source=images&cd=vfe&ved=0CAwQjRxqFwoTCMCrj_SgsfsCFQAAAAAdAAAAABAborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    -->
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <div class="content has-text-justified">
          <div class="container">
            <div style="display: flex; flex-direction: row; justify-content: center; align-items: center;">
              <span style="display: flex; flex-direction: column; align-items: center;">
                <image src="media/imgs/focus-string-square.jpg" style="height: 150px; width: auto;"/>
                <p class="image_caption">Picking up a moving ball</p>
              </span>
              <span>
                <image src="media/imgs/table_base.png" style="height: 200px;"/>
                <p class="image_caption"></p>
                
              </span>
              
            </div>
          </div>
          <p>
            We conduct extensive ablations in simulator and real world to provide 
            expirical evidence understanding our design choice.
          </p>
          <div>
            <p>
            </p>
            <div class="horizontal_list">
              <span class="list_image">
                <img src="media/imgs/demo.png" class="interpolation-image" alt="Egg Shell" style="max-width: 100%; max-height: 100%;" />
                <p class="image_caption">Offline Data</p>
              </span>
              <span class="list_image">
                <img src="media/imgs/latency.png" class="interpolation-image" alt="noodles" />
                <p class="image_caption">Randomize Latency</p>
              </span>
              <span class="list_image">
                <img src="media/imgs/layernorm.png" class="interpolation-image" alt="noodles" />
                <p class="image_caption">Layernorm</p>
              </span>
              <span class="list_image">
                <img src="media/imgs/utd.png" class="interpolation-image" alt="noodles" />
                <p class="image_caption">Update to Data</p>
              </span>
            </div>
            <p>
            </p>
            Our agent could generalize to varying perception noise; objects with different shape,
            size, texture; dynamic disturbance; objects with varying fluid support in <b>zero-shot
            , without further learning.
            </b>
          </div>
        <br/>
        <div  style="display: flex;
        justify-content: center;
        align-items: center">
        <img src="media/imgs/generalization.png" class="interpolation-image" 
         alt="Interpolate start reference image." class="center"/>
        </div>
        <br/>
        </div>
      </div>
      
    </div>
    <!--/ Abstract. -->

  </div>

    <!-- Paper video. -->
    <!--
    </br>
    </br>
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
        <h2 class="title is-3">Video</h2>
        <div class="publhttps://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.thekitchn.com%2Fnostress-solution-how-to-get-t-108285&psig=AOvVaw0y0Ig-QftM9qU6kjN_uCnS&ust=1668638482016000&source=images&cd=vfe&ved=0CAwQjRxqFwoTCMCrj_SgsfsCFQAAAAAdAAAAABAborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    -->
</section>

<section class="hero teaser">
  <div class="container">
    <div class="wrappable_list">
      <span>
        <image src="media/gifs/dynamixel_disturb_3.gif" alt="GIF of robot grabbing a ball that is being disturbed" />
      </span>
      <span>
        <image src="media/gifs/dynamixel_disturb_5.gif" alt="GIF of robot grabbing a ball that is being disturbed" />
      </span>
      <span>
        <image src="media/gifs/chess_piece.gif" alt="GIF of robot picking up a chess piece" />
      </span>
      <span>
        <image src="media/gifs/binder_clip.gif" alt="GIF of robot picking up a binder cip" />
      </span>
      <span>
        <image src="media/gifs/grab_from_cream.gif" alt="Gif of robot picking a cherry froma mound" />
      </span>
    </div>
  </div>

<script type="text/html"> <!-- COMMENT OUT-->

<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">


    <!-- Animation. -->
    <div class="rows is-centered ">
      <div class="row is-full-width">
        <h2 class="title is-3"><span class="dcliport">CLIPort</span></h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">Two-Stream Architecture</h3>
        <div class="content has-text-justified">
          <p>
            Broadly inspired by (or vaguely analogous to) the <a target=‚Äù_blank‚Äù href="https://en.wikipedia.org/wiki/Two-streams_hypothesis">two-stream hypothesis in cognitive psychology</a>, we present a two-stream architecture 
            for vision-based manipulation with semantic and spatial pathways. The semantic stream uses a pre-trained CLIP model  
            to encode RGB and language-goal input. Since CLIP is trained with large amounts of image-caption pairs from the internet,
            it acts as a powerful semantic prior for <a target="_blank" href="https://distill.pub/2021/multimodal-neurons/">grounding visual concepts</a> like colors, shapes, parts, texts, and object categories. 
            The spatial stream is a tabula rasa fully-convolutional network that encodes RGB-D input. 
          </p>
        </div>
        <img src="https://cliport.github.io/media/images/two_stream_architecture.png" class="interpolation-image" 
         alt="Interpolate start reference image." />
        <br/>
        <br/>
            <b>Paradigm 1:</b> Unlike existing object detectors, CLIP is not limited to a predefined set of object classes. And unlike other vision-language models, it's not restricted by a top-down pipeline that detects objects with bounding boxes or instance segmentations. This allows us to forgo the traditional paradigm of training explicit detectors for cloths, pliers, chessboard squares, cherry stems, and other arbitrary things. 
        <br/>
        <br/>
        <br/>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <h3 class="title is-4">TransporterNets</h3>
        <div class="content has-text-justified">
          <p>
            We use this two-stream architecture in all three networks of <a target=‚Äù_blank‚Äù href="https://transporternets.github.io/">TransporterNets</a> 
            to predict pick and place affordances at each timestep. TransporterNets first attends to a local region to decide where to pick, 
            then computes a placement location by finding the best match for the picked region through 
            cross-correlation of deep visual features. This structure serves as a powerful inductive bias for learning <a target="_blank" href="https://fabianfuchsml.github.io/equivariance1of2/">roto-translationally equivariant</a> representations in tabletop environments.

          </p>
        </div>
        <div class="content has-text-centered">
          <video id="transporter-gif"
                 controls
                 muted
                 autoplay
                 loop
                 width="40%">
            <source src="https://transporternets.github.io/images/animation.mp4"
                    type="video/mp4">
          </video>
          <p>
          Credit: <a href="https://transporternets.github.io/">Zeng et. al (Google)</a>
          </p>
        </div>
        <br/>
            <b>Paradigm 2:</b> TransporterNets takes an <a target="_blank" href="https://en.wikipedia.org/wiki/Ecological_psychology">action-centric approach</a> to perception where the objective is to <i>detect actions</i> rather than <i>detect objects</i> and then learn a policy. Keeping the action-space grounded in the perceptual input allows us to exploit geometric symmetries for efficient representation learning. 
            When combined with CLIP's pre-trained representations, this enables the learning of reusable manipulation skills without any "objectness" assumptions.
        <br/>
        <br/>
        <br/>

        <!--/ Re-rendering. -->

        <h2 class="title is-3">Results</h2>

        <div class="columns">
          <div class="column has-text-centered">
            <h3 class="title is-5">Single-Task Models</h3>

            Trained with
            <div class="select is-small">
              <select id="single-menu-demos" onchange="updateSingleVideo()">
              <option value="1">1</option>
              <option value="10">10</option>
              <option value="100">100</option>
              <option value="1000" selected="selected">1000</option>
              </select>
            </div>
            demos, evaluated on 
            <div class="select is-small">     
              <select id="single-menu-tasks" onchange="updateSingleVideo()">
              <option value="align-rope">align-rope</option>
              <option value="assembling-kits-seq-seen-colors">assembling-kits-seq-seen-colors</option>
              <option value="assembling-kits-seq-unseen-colors">assembling-kits-seq-unseen-colors</option>
              <option value="packing-boxes-pairs-seen-colors">packing-boxes-pairs-seen-colors</option>
              <option value="packing-boxes-pairs-unseen-colors">packing-boxes-pairs-unseen-colors</option>
              <option value="packing-seen-google-objects-seq" selected="selected">packing-seen-google-objects-seq</option>
              <option value="packing-unseen-google-objects-seq">packing-unseen-google-objects-seq</option>
              <option value="packing-seen-google-objects-group">packing-seen-google-objects-group</option>
              <option value="packing-unseen-google-objects-group">packing-unseen-google-objects-group</option>
              <option value="packing-shapes">packing-shapes</option>
              <option value="put-block-in-bowl-seen-colors">put-block-in-bowl-seen-colors</option>
              <option value="put-block-in-bowl-unseen-colors">put-block-in-bowl-unseen-colors</option>
              <option value="separating-piles-seen-colors">separating-piles-seen-colors</option>
              <option value="separating-piles-unseen-colors">separating-piles-unseen-colors</option>
              <option value="stack-block-pyramid-seq-seen-colors">stack-block-pyramid-seq-seen-colors</option>
              <option value="stack-block-pyramid-seq-unseen-colors">stack-block-pyramid-seq-unseen-colors</option>
              <option value="towers-of-hanoi-seq-seen-colors">towers-of-hanoi-seq-seen-colors</option>
              <option value="towers-of-hanoi-seq-unseen-colors">towers-of-hanoi-seq-unseen-colors</option>
              </select>
            </div>
            instance
            <div class="select is-small">
              <select id="single-menu-instances" onchange="updateSingleVideo()">
              <option value="01">01</option>
              <option value="02">02</option>
              <option value="03">03</option>
              <option value="04">04</option>
              <option value="05" selected="selected">05</option>
              </select>
            </div>
            <br/>
            <br/>

            <video id="single-task-result-video"
                   controls
                   muted
                   autoplay
                   loop
                   width="100%">
              <source src="https://homes.cs.washington.edu/~mshr/cliport/results_web/packing-seen-google-objects-seq-two_stream_full_clip_lingunet_lat_transporter-n1000-train/videos/packing-seen-google-objects-seq-000005.mp4"
                      type="video/mp4">
            </video>
          </div>

          <div class="column has-text-centered">
            <h3 class="title is-5">One Multi-Task Model</h3>
            
            Trained with
            <div class="select is-small">
              <select id="multi-menu-demos" onchange="updateMultiVideo()">
              <option value="1">1 T</option>
              <option value="10">10 T</option>
              <option value="100">100 T</option>
              <option value="1000" selected="selected">1000 T</option>
              </select>
            </div>
            demos, evaluated on  
            <div class="select is-small">   
              <select id="multi-menu-tasks" onchange="updateMultiVideo()">
              <option value="align-rope">align-rope</option>
              <option value="assembling-kits-seq-seen-colors">assembling-kits-seq-seen-colors</option>
              <option value="assembling-kits-seq-unseen-colors">assembling-kits-seq-unseen-colors</option>
              <option value="packing-boxes-pairs-seen-colors" selected="selected">packing-boxes-pairs-seen-colors</option>
              <option value="packing-boxes-pairs-unseen-colors">packing-boxes-pairs-unseen-colors</option>
              <option value="packing-seen-google-objects-seq">packing-seen-google-objects-seq</option>
              <option value="packing-unseen-google-objects-seq">packing-unseen-google-objects-seq</option>
              <option value="packing-seen-google-objects-group">packing-seen-google-objects-group</option>
              <option value="packing-unseen-google-objects-group">packing-unseen-google-objects-group</option>
              <option value="packing-shapes">packing-shapes</option>
              <option value="put-block-in-bowl-seen-colors">put-block-in-bowl-seen-colors</option>
              <option value="put-block-in-bowl-unseen-colors">put-block-in-bowl-unseen-colors</option>
              <option value="separating-piles-seen-colors">separating-piles-seen-colors</option>
              <option value="separating-piles-unseen-colors">separating-piles-unseen-colors</option>
              <option value="stack-block-pyramid-seq-seen-colors">stack-block-pyramid-seq-seen-colors</option>
              <option value="stack-block-pyramid-seq-unseen-colors">stack-block-pyramid-seq-unseen-colors</option>
              <option value="towers-of-hanoi-seq-seen-colors">towers-of-hanoi-seq-seen-colors</option>
              <option value="towers-of-hanoi-seq-unseen-colors">towers-of-hanoi-seq-unseen-colors</option>
              </select>
            </div>
            instance
            <div class="select is-small">
              <select id="multi-menu-instances" onchange="updateMultiVideo()">
              <option value="01">01</option>
              <option value="02">02</option>
              <option value="03">03</option>
              <option value="04" selected="selected">04</option>
              <option value="05">05</option>
              </select>
            </div>
            </br>
            </br>

            <video id="multi-task-result-video"
                   controls
                   muted
                   autoplay
                   loop
                   width="100%">
              <source src="https://homes.cs.washington.edu/~mshr/cliport/results_web/packing-boxes-pairs-seen-colors-two_stream_full_clip_lingunet_lat_transporter-n1000-train/videos/multi-language-conditioned-packing-boxes-pairs-seen-colors-000004.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
        </br>

        <h3 class="title is-4">Affordance Predictions</h3>
        <div class="content has-text-justified">
          <p>
            Examples of pick and place affordance predictions from multi-task <span class="dcliport">CLIPort</span> models:
          </p>
        </div>
        <br/>
        <img src="https://cliport.github.io/media/images/affordances.png" class="interpolation-image" 
         alt="Interpolate start reference image."/>
        <br/>
        <br/>
        <img src="https://cliport.github.io/media/images/affordance2.png" class="interpolation-image" 
         alt="Interpolate start reference image."/>

      </div>
    </div>

  </div>
</section>
</script>


<script type="text/html"> <!-- COMMENT OUT-->
<section class="section" id="BibTeX">
  <div class="container is-max-widescreen content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{shridhar2021cliport,
  title     = {CLIPort: What and Where Pathways for Robotic Manipulation},
  author    = {Shridhar, Mohit and Manuelli, Lucas and Fox, Dieter},
  booktitle = {Proceedings of the 5th Conference on Robot Learning (CoRL)},
  year      = {2021},
}</code></pre>
  </div>
</section>
</script>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> made by the amazing <a href="https://keunhong.com/">Keunhong Park</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
